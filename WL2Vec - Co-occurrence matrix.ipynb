{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ab49a2-f8b9-42d9-a391-0ddf32853393",
   "metadata": {},
   "source": [
    "# Load data from preprocessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a9b0ae-0ba8-4e7d-a225-a97f45904fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('WL_sent_filtered.pkl', 'rb') as f:\n",
    "    WL_sent_filtered = pickle.load(f)\n",
    "\n",
    "with open('all_words.pkl', 'rb') as f:\n",
    "    all_words = pickle.load(f)\n",
    "\n",
    "#with open('word_counts.pkl', 'rb') as f:\n",
    "#    word_counts = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-guatemala",
   "metadata": {},
   "source": [
    "# Creating Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "micro-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('verfasser', 'unter'), ('verfasser', 'wissenschaftslehre'), ('verfasser', 'verstehe'), ('unter', 'verfasser'), ('unter', 'wissenschaftslehre'), ('unter', 'verstehe'), ('wissenschaftslehre', 'verfasser'), ('wissenschaftslehre', 'unter'), ('wissenschaftslehre', 'verstehe'), ('verstehe', 'verfasser')]\n",
      "3039692\n"
     ]
    }
   ],
   "source": [
    "################# I use sliding-window co-occurrence, respecting sentence boundaries, for this is Word2Vec-like and should capture both syntactic and semantic structure without being too computationally heavy. \n",
    "\n",
    "window_size = 5\n",
    "\n",
    "bigrams = []\n",
    "\n",
    "for sent in WL_sent_filtered:\n",
    "    for i, word in enumerate(sent):\n",
    "        for j in range(max(0, i - window_size), min(len(sent), i + window_size + 1)): # looks at the window_size words before and after the word in position i, while staying within the sentence boundaries\n",
    "            if i != j: # exclude the target word itself from its window\n",
    "                bigrams.append((word, sent[j]))\n",
    "                \n",
    "\n",
    "print(bigrams[:10])\n",
    "print(len(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895eeef8-629e-4ef3-938f-3536c0e9be48",
   "metadata": {},
   "source": [
    "# Filter bigrams for frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36febf19-aade-43ef-a602-c53d193e859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219534\n",
      "[('verfasser', 'unter'), ('verfasser', 'verstehe'), ('unter', 'verfasser'), ('unter', 'wissenschaftslehre'), ('unter', 'verstehe'), ('wissenschaftslehre', 'unter'), ('wissenschaftslehre', 'verstehe'), ('verstehe', 'verfasser'), ('verstehe', 'unter'), ('verstehe', 'wissenschaftslehre'), ('vorstelle', 'alle'), ('alle', 'vorstelle'), ('alle', 'wahrheiten'), ('alle', 'mensch'), ('alle', 'kennt'), ('wahrheiten', 'alle'), ('wahrheiten', 'hat'), ('mensch', 'alle'), ('mensch', 'hat'), ('kennt', 'alle')]\n"
     ]
    }
   ],
   "source": [
    "minfreq = 3 # Gives 219534, which seems like a good number\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "bigrams_count = Counter(bigrams) # = Dictionary-like object\n",
    "\n",
    "freq_bigrams = []\n",
    "\n",
    "for bigram, count in bigrams_count.items():\n",
    "    if count >= minfreq:\n",
    "        freq_bigrams.append(bigram)\n",
    "\n",
    "print(len(freq_bigrams))\n",
    "print(list(freq_bigrams)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-ebony",
   "metadata": {},
   "source": [
    "# Create matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "critical-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Create index\n",
    "\n",
    "word2idx = {}\n",
    "for i, word in enumerate(all_words):\n",
    "    word2idx[word] = i\n",
    "\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "################### Create matrix. Sparse for computability. \n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "cooc_mat = lil_matrix((vocab_size, vocab_size), dtype=int)\n",
    "\n",
    "for bigram in freq_bigrams:\n",
    "    w1 = bigram[0]\n",
    "    w2 = bigram[1]\n",
    "    \n",
    "    if w1 in word2idx and w2 in word2idx:\n",
    "        i = word2idx[w1]\n",
    "        j = word2idx[w2]\n",
    "        cooc_mat[i, j] += 1 # counts how many times word_i co-occurs with word_j\n",
    "\n",
    "# Convert to csr format for efficiency\n",
    "cooc_mat = cooc_mat.tocsr()\n",
    "\n",
    "################## Apply dimensionality reduction (Truncated SVD)\n",
    "# Truncated SVD = Singular Value Decomposition (breaking matrix into orthogonal components that capture the main patterns in the data), keeping only\n",
    "# the top k singular values/components (=n_components), which gives a dense, low-dimensional embedding for each word\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components = 100  # choose embedding size\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "WL_embedding = svd.fit_transform(cooc_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06c047-bf31-432e-96ec-e79cdf05e9d4",
   "metadata": {},
   "source": [
    "# Inspecting the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63ed5345-9927-455e-95eb-7b1955102026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for wissenschaft : [19.76114762 -7.42731554 -0.13690904  6.67732679  0.96853948 -1.37987829\n",
      "  0.36439024  4.25679079  0.26488511  5.4158889  -3.19272996 -1.63554292\n",
      "  0.11766309  0.54437407 -1.13362506  1.5867999   1.72924777  0.59082688\n",
      "  3.42506758 -1.78426044 -7.1678819  -1.77675753  0.91274625 -0.38087625\n",
      "  0.90779784  0.94896927 -0.65997781 -2.28736498  2.22346766 -1.59176328\n",
      " -0.1583982   1.8878968  -1.73249522  0.09682568 -2.96105938 -1.67580532\n",
      "  0.75513875 -4.27838253  3.01867792  1.70657818  2.21885707 -0.21780456\n",
      " -0.11182947  0.88032052 -0.79345423  2.14622736  0.72860059  0.89635395\n",
      " -1.90096562 -0.52410769 -1.87932803 -2.46130842  1.60133148 -0.9364986\n",
      " -0.40779105 -1.41799584  1.14830668 -0.75978281 -0.11391834  0.80148087\n",
      " -0.06619663  2.46513386  2.78842432 -0.55329733  0.27847198  0.98722244\n",
      "  1.12527973  2.02053962 -2.22393926  0.67640507  1.87250182  0.98808145\n",
      "  1.80602033 -2.04007604  1.34011922 -1.1997673   0.90227378 -0.7580385\n",
      " -0.7449035  -0.10905966 -1.07016599 -1.84979812  1.52757776 -0.4132236\n",
      "  0.73802319  0.83269898 -0.33014685  0.30485159  0.43156534  1.78160879\n",
      " -0.02373187  1.4428995  -3.1016491   0.05955223 -1.31747078 -2.04358034\n",
      "  1.18611629  1.20020597  0.60728113  0.17260808]\n"
     ]
    }
   ],
   "source": [
    "################### Show vector for target word (mind: lower case!)\n",
    "\n",
    "target_word = \"wissenschaft\"\n",
    "\n",
    "idx = word2idx[target_word]\n",
    "print(\"Vector for\", target_word, \":\", WL_embedding[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90325ee2-5c5b-4121-badf-8533fde6e94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 closest words: ['wahrheiten', 'lehren', 'finden', 'logik', 'jeden']\n"
     ]
    }
   ],
   "source": [
    "################# Find similar words (using cosine similarity)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similar = cosine_similarity([WL_embedding[idx]], WL_embedding)[0]\n",
    "\n",
    "##### Exclude the word itself\n",
    "similar[idx] = -1  # or any value lower than possible similarities\n",
    "\n",
    "####### Get top 5 closest words\n",
    "\n",
    "top_idx = []\n",
    "\n",
    "for i in range(5):\n",
    "    max_sim = max(similar)\n",
    "    max_idx = similar.tolist().index(max_sim)\n",
    "    top_idx.append(max_idx)\n",
    "    similar[max_idx] = -1  #  mark as used\n",
    "\n",
    "print(\"Top 5 closest words:\", [all_words[i] for i in top_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f06e3119828c145c4028f0fee76387e74b42caa79bccb80b4ba64fe213bb9e06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
